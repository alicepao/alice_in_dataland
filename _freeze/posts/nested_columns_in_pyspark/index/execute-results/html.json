{
  "hash": "2610cbe32cd57902bc4615f478e9b9f3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Nested Columns in Pyspark\"\nauthor: \"Alice Pao\"\ndate: \"2024-02-24\"\ncategories: [tutorial, code, polars]\nimage: \"parquet.jpg\"\n---\n\nWhen dealing with great volume of data which we often refer to as \"big data\", it is common to have nested columns. They not only help with data format but also save a lot of memory. One of the most popular file formats that use nested columns is Parquet. Therefore, before jumping into nested columns, I want to spend some time explaining what Parquet is. \n\n\n## What is Parquet? \n[Apache Parquet](https://www.databricks.com/glossary/what-is-parquet), mostly called as Parquet, is an open source file format that saves column-based data, created by databricks. It provides efficient data storeage and retrival. It is used to handle complex data structure and it compresses and decompresses data in a more efficient way compared to other traditional file formats like CSV. \n\nParquet can store data including images, tables, videos, and documents. One of its benefits is its ability to skip data, meaning when data is read in by queries, instead of reading the entire table, it only grabs the specific columns where the queries specifiy. This saves a lot of data processing time. \n\nBelow is taken from [databricks](https://www.databricks.com/glossary/what-is-parquet). It compares Parquet with CVS. We can see that using Parquet file format is both cheaper and faster. Therefore, I highly recommend data science professionals familarize themselves with Parquet! \n\n![](parquet_vs_cvs.png)\n\n\n## What is Nested Columns? \nNow, let's talk about about nested columns. Just like the words suggest, a nested column is a column that has multiple comlumns nested in it. (I swear this is not a tongue twister!) You may ask, why do we need it? Well, having nested columns give us more flexibility as we can add additional columns into existing ones, which gives more control when we are dealing with complex data. \n\nHere is an example of normal columns: \n\n![](normal%20column.png)\n\n\nThis one takes the `first name`, `middle name`, and `last name` columns and nest them into a `name` column. \n\n![](nested_columns.png)\n\nBy doing so, we make a cleaner and more organized data structure and also save some storage space and data retrieval time. \n\n\n## Different Types of Nested Columns in Pyspark\nThere are different types of nested columns we can utilize in Pyspark. \n\n* ArrayType\nIt is an array of data type. Users can store a collection of same type of elements. \n\n* MapType\nIt is a map of key value pairs in a DataFrame. Users can store key-value mappings. It is similar to a dictionary in python. \n\n* StructType\nIt is a list or tuple value type in Python. It stores a collection of StructField. \n\n## How to Create Nested Columns? \n\nBefore creating any of these datatypes, we need to import the following function from pyspark. \n\n::: {#f9c04e78 .cell execution_count=1}\n``` {.python .cell-code}\n# from pyspark.sql.types import *\n```\n:::\n\n\nHere are the parameters for each of the three data types: \n\n* ArrayType: \n1. elementType: Defines the DataType for all elements in the array\n2. containsNull: This is an optional parameter. It declares whether the array can have null values or not. \n\n::: {#c5345c6e .cell execution_count=2}\n``` {.python .cell-code}\n# array_data = [(3500, 5000, 8000, 2500, 4000)]\n# array_schema = ArrayType(StringType())\n# dataframe = spark.createDataFrame(data = array_data, schema = array_schema)\t\n# dataframe.printSchema()\t\n# dataframe.show(truncate=False)\t\n```\n:::\n\n\nThe Result: \n![](array_example.png)\n\n\n* MapType: \n1. keyType: Defines the DataType for the keys in the map. \n2. valueType: Defines the DataType for the values in the map. \n3. valueContainsNull: This is an optional parameter. It declares whether the map can have null values or not. \n\n::: {#706c81b4 .cell execution_count=3}\n``` {.python .cell-code}\n# map_data = [('graphic designer', 3500), ('junior project manager', 5000), ('senior project manager', 8000), ('office assistant', 2500), ('junior data analyst', 4000)]\n# map_schema = MapType(StringType(), IntegerType())\n# dataframe = spark.createDataFrame(data = map_data, schema = map_schema)\t\n# dataframe.printSchema()\t\n# dataframe.show(truncate=False)\t\n```\n:::\n\n\n* StructType: \nBefore talking about StructType, we need to first understand StructField. StructField is an object that consists of name(a string), dataType, and the nullable. Here's an example: \n\n::: {#bb206ad6 .cell execution_count=4}\n``` {.python .cell-code}\n# StructField(\"first_name\", StringType(), True)\n```\n:::\n\n\nStructType is created by calling the StructField. For example: \n\n::: {#af49aacf .cell execution_count=5}\n``` {.python .cell-code}\n# Structure_Data = [((\"Katy\",\"\",\"Wellen\"),\"11111\",\"F\",3500),\t\n#     ((\"Samuel\",\"Komo\",\"\"),\"22222\",\"M\",5000),\t\n#     ((\"Luke\",\"\",\"Hitch\"),\"33333\",\"M\",8000),\t\n#     ((\"Mike\",\"Smith\",\"Nelson\"),\"44444\",\"M\",2500),\t\n#     ((\"Cathy\",\"Reid\",\"Brown\"),\"\",\"F\",4000)\t\n#   ]\t\n\n# Structure_Schema = StructType([\t\n#         StructField('name', StructType([\t\n#              StructField('first_name', StringType(), True),\t\n#              StructField('middle_name', StringType(), True),\t\n#              StructField('last_name', StringType(), True)\t\n#              ])),\t\n#          StructField('id', StringType(), True),\t\n#          StructField('gender', StringType(), True),\t\n#          StructField('salary', IntegerType(), True)\t\n#          ])\n\n# dataframe = spark.createDataFrame(data = Structure_Data, schema = Structure_Schema)\n# dataframe.printSchema()\t\n# dataframe.show(truncate=False)\t\n```\n:::\n\n\n## How to \"Unnest\"?\n\n\n\n\n## Conclusion\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}